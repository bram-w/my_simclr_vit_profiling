{
 "cells": [
  {
   "cell_type": "raw",
   "id": "25cea1c6",
   "metadata": {},
   "source": [
    "pip install braceexpand\n",
    "pip install timm==0.6.13\n",
    "pip install tensorboardX\n",
    "pip install ftfy\n",
    "pip install regex\n",
    "pip install setuptools==59.5.0\n",
    "pip install transformers==4.27.2\n",
    "pip install diffusers==0.14.0\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "pip install torchmetrics\n",
    "pip install open_clip_torch\n",
    "pip install pycocotools\n",
    "pip uninstall safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a50bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import multimodal\n",
    "\n",
    "\n",
    "# a bunch of below is unncessary but was in example of https://github.com/christophschuhmann/improved-aesthetic-predictor/blob/main/sac%2Blogos%2Bava1-l14-linearMSE.pth\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    # choose GPU if you are on a multi GPU server\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "\n",
    "from os.path import join\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "import clip\n",
    "import time\n",
    "\n",
    "\n",
    "from PIL import Image, ImageFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(\"cuda\").manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(batch_size=1):\n",
    "    generator = [torch.Generator(\"cuda\").manual_seed(i) for i in range(batch_size)]\n",
    "    prompts = batch_size * [prompt]\n",
    "    num_inference_steps = 20\n",
    "\n",
    "    return {\"prompt\": prompts, \"generator\": generator, \"num_inference_steps\": num_inference_steps}\n",
    "\n",
    "def image_grid(imgs, rows=2, cols=2):\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4d82539",
   "metadata": {},
   "source": [
    "# pipeline.enable_attention_slicing()\n",
    "prompt = \"An oil painting of a pirate ship battle\"\n",
    "images = pipeline(**get_inputs(batch_size=16)).images\n",
    "image_grid(images, rows=4, cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2765ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)  #RN50x64   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, xcol='emb', ycol='avg_rating'):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.xcol = xcol\n",
    "        self.ycol = ycol\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(64, 16),\n",
    "            #nn.ReLU(),\n",
    "\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "\n",
    "aes_model = MLP(768)  # CLIP embedding dim is 768 for CLIP ViT L 14\n",
    "s = torch.load(\"sac+logos+ava1-l14-linearMSE.pth\")   # load the model you trained previously or the model available in this repo\n",
    "aes_model.load_state_dict(s)\n",
    "aes_model.to(\"cuda\")\n",
    "aes_model.eval()\n",
    "\n",
    "\n",
    "def aesthetic_score(image_tensor): # image tensors as below\n",
    "    image_feats = clip_model.encode_image(image_tensor)\n",
    "    image_emb_arr = image_feats / image_feats.norm(dim=1, keepdim=True)\n",
    "    return aes_model(image_emb_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a53a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef413b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ce401",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A cute sloth holding a small treasure chest. A bright golden glow is coming from the chest.\"\n",
    "n_show = 3\n",
    "aes_weight = 1\n",
    "\n",
    "start_time = time.time()\n",
    "images = pipeline(**get_inputs(batch_size=16)).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_images = torch.cat([preprocess(im).unsqueeze(0).to(device) for im in images])\n",
    "text = clip.tokenize([prompt]).to(device)\n",
    "logits_per_image, logits_per_text = clip_model(feed_images, text)\n",
    "with torch.cuda.amp.autocast():\n",
    "    aes_scores = aesthetic_score(feed_images)   \n",
    "x = logits_per_image.flatten().cpu().numpy()\n",
    "y = aes_scores.flatten().cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "for i, (x_pos, y_pos) in enumerate(zip(x,y)):\n",
    "    plt.text( x_pos, y_pos, str(i))\n",
    "plt.xlabel('CLIP Score')\n",
    "plt.ylabel('Aes. Score')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "idx_rank = np.argsort(x+y*aes_weight)[::-1]\n",
    "print(idx_rank)\n",
    "\n",
    "best_idx = idx_rank[:n_show]\n",
    "worst_idx = idx_rank[-n_show:]\n",
    "\n",
    "for idx_set in [best_idx, worst_idx]:\n",
    "    for idx in idx_set:\n",
    "        print(idx, x[idx], y[idx])\n",
    "    display(image_grid([images[i] for i in idx_set], rows=1, cols=n_show))\n",
    "    \n",
    "print(\"Total time:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f178fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
