{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, CocoCaptions, ImageNet\n",
    "import slip_models\n",
    "from tokenizer import SimpleTokenizer\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "%pip install ipywidgets\n",
    "%pip install update tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = T.Compose(\n",
    "                    [\n",
    "                        T.Resize(224),\n",
    "                        T.CenterCrop(224),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = slip_models.CLIP_VITB16(embed_dim=8)\n",
    "# ckpt = torch.load('ckpts/clip_embed_dim_8_epoch_35.ckpt', map_location='cpu')\n",
    "\n",
    "model = slip_models.VisionParallelTextStandard()\n",
    "ckpt = torch.load('ckpts/test_experimental_epoch_2.ckpt', map_location='cpu')\n",
    "\n",
    "# model = slip_models.CLIP_VITB16(num_prompt_tokens=64, num_text_outputs=1000)\n",
    "# ckpt = torch.load('ckpts/epoch_30_prompted_clip_may_27.ckpt', map_location='cpu')\n",
    "\n",
    "\n",
    "model.load_state_dict({k.replace('module.',''):v for k,v in ckpt[\"model\"].items()})\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152dd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = preprocess(Image.open(\"pics/golden-retriever.png\")).unsqueeze(0).to(device)\n",
    "# image = preprocess(Image.open(\"pics/CLIP.png\")).unsqueeze(0).to(device)\n",
    "base_text = [\"a diagram\", \"a dog\", \"a cat\"]\n",
    "model.eval()\n",
    "dog_image = preprocess(Image.open(\"pics/golden-retriever.png\"))\n",
    "diagram_image = preprocess(Image.open(\"pics/CLIP.png\").convert(\"RGB\"))\n",
    "cat_image = preprocess(Image.open(\"pics/cat.jpg\"))\n",
    "images = torch.stack([diagram_image, dog_image, cat_image]).cuda()\n",
    "\n",
    "text = tokenizer([f\"a picture of {s}\" for s in base_text]).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(images)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image = model.logit_scale.exp() * image_features @ text_features.t()\n",
    "    # logits_per_image, logits_per_text = model(images, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed2a0c",
   "metadata": {},
   "source": [
    "# Basic ImageNet and CIFAR checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f518e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "cifar10 = CIFAR10(root=\"/tmp/\", transform=preprocess, download=True, train=False)\n",
    "loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "# text_inputs = torch.stack([tokenizer(f\"a photo of a {c}\") for c in cifar10.classes]).cuda()\n",
    "\n",
    "\n",
    "text_inputs = torch.stack([tokenizer(f\"a photo of a {c}\") for c in cifar10.classes]).cuda()\n",
    "\n",
    "\n",
    "num_correct = 0\n",
    "num_seen = 0\n",
    "with torch.no_grad():\n",
    "    #  text_features = model.encode_text(text_inputs)\n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        # image_features = model.encode_image(imgs)\n",
    "        # results = model(imgs, text_inputs, lang_prompt_viz=True, sharded_computation=False)\n",
    "        results = model(imgs, text_inputs)\n",
    "        image_features = results['image_embed']\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        text_features = results['text_embed']\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        max_sims = similarity.max(dim=-1)[0]\n",
    "        print(max_sims.min(), max_sims.max(), max_sims.mean())\n",
    "        num_correct += (similarity.argmax(dim=-1)==targets).sum().item()  \n",
    "        num_seen += imgs.shape[0]\n",
    "        curr_acc = num_correct / num_seen\n",
    "        print(f\"Current Acc: {curr_acc}\")\n",
    "acc = num_correct / len(cifar10)\n",
    "print(f\"Final Acc {acc}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e35fbfd8",
   "metadata": {},
   "source": [
    "epoch 35 of my orig CLIP model (both vanilla prompting)\n",
    "CIFAR 69.2%\n",
    "ImageNEt 28.2\n",
    "\n",
    "\n",
    "epoch 2 of parallel vision standard text got 24% so definitely nontrivial signal\n",
    "Also got 7.3% on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3de913",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee44dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imagenet = ImageNet(root=\"/export/share/datasets/vision/imagenet\", transform=preprocess, split='val')\n",
    "loader = torch.utils.data.DataLoader(imagenet, batch_size=32, num_workers=4)\n",
    "text_inputs = torch.stack([tokenizer(f\"a photo of a {c}\") for c in imagenet.classes]).cuda()\n",
    "\n",
    "num_correct = 0\n",
    "num_seen = 0\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        image_features = model.encode_image(imgs)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        # results = model(imgs, text_inputs, lang_prompt_viz=True, sharded_computation=False)\n",
    "        # image_features = results['image_embed']\n",
    "        # text_features = results['text_embed']\n",
    "        \n",
    "        \n",
    "        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        num_correct += (similarity.argmax(dim=-1)==targets).sum().item()  \n",
    "        max_sims = similarity.max(dim=-1)[0]\n",
    "        # print(max_sims.min(), max_sims.max(), max_sims.mean())\n",
    "        num_seen += imgs.shape[0]\n",
    "        curr_acc = num_correct / num_seen\n",
    "        print(f\"Current Acc: {curr_acc}\")\n",
    "        \n",
    "print(f\"Final Acc {curr_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8562ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist( (text_sims.cpu()).flatten().detach().cpu().numpy(), label='text sims', alpha=0.5)\n",
    "plt.hist( (image_sims.cpu()).flatten().detach().cpu().numpy(), label='image_sims', alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2cdbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9564c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd182a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "def dataset_constructor(name, split, transform):\n",
    "    \"\"\"\n",
    "    inputs\n",
    "        name: string that we'll run if/else on\n",
    "        split: \"train\" or \"val\" for now\n",
    "    \"\"\"\n",
    "    assert split in ['train', 'val']\n",
    "\n",
    "    # Need to figure out way to override the getitem call for strong weak augmentatin\n",
    "    if name == 'cifar10':\n",
    "        # Has class names\n",
    "        return datasets.CIFAR10(root=os.path.expanduser(\"~/.cache\"),\n",
    "                                download=True,\n",
    "                                train=(split=='train'),\n",
    "                               transform=transform)\n",
    "    elif name == 'cifar100':\n",
    "        # Has class names\n",
    "        return datasets.CIFAR100(root=os.path.expanduser(\"~/.cache\"),\n",
    "                                download=True,\n",
    "                                train=(split=='train'),\n",
    "                                transform=transform)\n",
    "\n",
    "    elif name == 'svhn':\n",
    "        # Doesn't have class names\n",
    "        return datasets.SVHN(root=os.path.expanduser(\"~/.cache\"),\n",
    "                             download=True,\n",
    "                             transform=transform,\n",
    "                             split=split)\n",
    "    elif name == 'food101':\n",
    "        # name is in .classes\n",
    "        assert split=='val'\n",
    "        return datasets.ImageFolder(root=\"/export/share/bwallace/datasets/food101/images/\",\n",
    "                   transform=transform)\n",
    "    elif name == 'merced':\n",
    "        # name is in .classes\n",
    "        assert split=='val'\n",
    "        return datasets.ImageFolder(root=\"/export/share/bwallace/datasets/UCMerced_LandUse/Images/\",\n",
    "                   transform=transform)\n",
    "    elif name == 'dtd':\n",
    "        # name is in .classes\n",
    "        return datasets.ImageFolder(root=f\"/export/share/bwallace/datasets/dtd/{split}\",\n",
    "                   transform=transform)\n",
    "    elif name == 'cub':\n",
    "        # name is in .classes\n",
    "        return datasets.ImageFolder(root=f\"/export/share/bwallace/datasets/CUB_2011_formatted/{split}\",\n",
    "                   transform=transform)\n",
    "    elif name == 'places365':\n",
    "        # class names works in \n",
    "        return datasets.Places365(\n",
    "                    root=\"/export/share/datasets/vision/Places365/\",\n",
    "                    split='train-standard' if split=='train' else split,\n",
    "                   transform=transform)\n",
    "\n",
    "    elif name == 'imagenet':\n",
    "        # Doesn't have class names, have separate call\n",
    "        return datasets.ImageNet(\n",
    "                    root=\"/export/share/datasets/vision/imagenet/\",\n",
    "                    split=split,\n",
    "                   transform=transform)\n",
    "    elif name == 'imagenet_val':\n",
    "        assert split=='train'\n",
    "        # Doesn't have class names, only use is having Imagenet val be trainset \n",
    "        return datasets.ImageNet(\n",
    "                    root=\"/export/share/datasets/vision/imagenet/\",\n",
    "                    split='val',\n",
    "                   transform=transform)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_imagenet_class_dict():\n",
    "        idx_to_word_id_and_name_tuple = json.load(open('imagenet_class_index.json'))\n",
    "        word_id_to_name_and_idx = {v[0]:(v[1],int(k))\n",
    "                           for k,v in idx_to_word_id_and_name_tuple.items()}\n",
    "        # e.g. n0023923939 to ('unicorn', 123)\n",
    "        return word_id_to_name_and_idx"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fceb0093",
   "metadata": {},
   "source": [
    "X Custom prompt\n",
    "\n",
    "Option for basic or importance masking\n",
    "    Need to play with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e5740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_dataset(model, dataset_name, transform,\n",
    "                   prompt_template=\"a photo of a {}\",\n",
    "                   test_components_mode='standard',\n",
    "                   normalization_from_all_components=False):\n",
    "    \"\"\"\n",
    "    Options for test components mode\n",
    "        standard : standard\n",
    "        test_all : scale through\n",
    "        random_all : As all but random sorting\n",
    "        \n",
    "    Normalization is whether to normalize from all components or just ones in computation\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = dataset_constructor(dataset_name, 'val', transform)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "    class_names = [prompt_template.format(c.replace(\"'\",\"\")) for c in dataset.classes]\n",
    "    \n",
    "    # print(class_names)\n",
    "    text_inputs = torch.stack([tokenizer(txt) for txt in class_names]).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        \n",
    "        if test_components_mode == 'standard':\n",
    "            sorted_feature_variances, sorted_variance_idx = None, None\n",
    "        elif test_components_mode == 'test_all':\n",
    "            sorted_feature_variances, sorted_variance_idx = sorted_feature_component_variances(text_features)\n",
    "        elif test_components_mode == 'random_all':\n",
    "            sorted_feature_variances, sorted_variance_idx = sorted_feature_component_variances(text_features)\n",
    "            sorted_variance_idx = torch.randperm(sorted_variance_idx.shape[0])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        num_correct = 0 if test_components_mode =='standard' else torch.zeros(text_features.shape[1])\n",
    "        num_seen = 0\n",
    "        for imgs, targets in tqdm(loader):\n",
    "            imgs = imgs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            image_features = model.encode_image(imgs)\n",
    "            image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "            # results = model(imgs, text_inputs, lang_prompt_viz=True, sharded_computation=False)\n",
    "            # image_features = results['image_embed']\n",
    "            # text_features = results['text_embed']\n",
    "\n",
    "\n",
    "            if test_components_mode==\"standard\":\n",
    "                similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "                num_correct += (similarity.argmax(dim=-1)==targets).sum().item() \n",
    "            elif test_components_mode in [\"test_all\", \"random_all\"]:\n",
    "                for num_components in range(1, text_features.shape[1]+1):\n",
    "                    idx_to_use = sorted_variance_idx[:num_components]\n",
    "                    masked_text_features = text_features[:, idx_to_use]\n",
    "                    masked_image_features = image_features[:, idx_to_use]\n",
    "                    if not normalization_from_all_components:\n",
    "                        masked_text_features =  masked_text_features / masked_text_features.norm(dim=1, keepdim=True)\n",
    "                        masked_image_features =  masked_image_features / masked_image_features.norm(dim=1, keepdim=True)\n",
    "                    masked_similarity = (masked_image_features @ masked_text_features.T).softmax(dim=-1)\n",
    "                    num_correct[num_components-1] += (masked_similarity.argmax(dim=-1)==targets).sum().item()\n",
    "                    \n",
    "            # max_sims = similarity.max(dim=-1)[0]\n",
    "            # print(max_sims.min(), max_sims.max(), max_sims.mean())\n",
    "            num_seen += imgs.shape[0]\n",
    "            curr_acc = num_correct / num_seen\n",
    "    return curr_acc\n",
    "\n",
    "\n",
    "def get_dataset_text_features(model, dataset_name):\n",
    "    dataset = dataset_constructor(dataset_name, 'val', None)\n",
    "    class_names = [f\"a photo of a {c}\" for c in dataset.classes]\n",
    "    text_inputs = torch.stack([tokenizer(txt) for txt in class_names]).cuda()\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    return text_features\n",
    "\n",
    "def sorted_feature_component_variances(tensor):\n",
    "    # NOTE: sorting max first so [0] is intuitively important\n",
    "    feature_variances = tensor.var(dim=0)\n",
    "    idx = feature_variances.argsort().flip(0)\n",
    "    return feature_variances[idx], idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f59e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_dataset(model, 'dtd', preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_dataset(model, 'cub', preprocess, test_components_mode='test_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a275ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_dataset(model, 'cub', preprocess, test_components_mode='random_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_dataset(model, 'cub', preprocess, test_components_mode='random_all')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37ae195e",
   "metadata": {},
   "source": [
    "Looks promising! \n",
    "\n",
    "Signal of the sorted lower components being superior to random is looking decent\n",
    "\n",
    "CAVEAT: STILL USING FULL IMAGE NORMALIZATOIN\n",
    "\n",
    "Note better when not all typically ?\n",
    "DTd\n",
    "With sorting (still just epoch 2\n",
    "tensor([0.0330, 0.0346, 0.0330, 0.0420, 0.0388, 0.0378, 0.0441, 0.0495, 0.0596,\n",
    "        0.0590, 0.0612, 0.0633, 0.0606, 0.0612, 0.0564, 0.0564, 0.0606, 0.0622,\n",
    "        0.0644, 0.0559, 0.0601, 0.0580, 0.0559, 0.0537, 0.0601, 0.0638, 0.0707,\n",
    "        0.0723, 0.0750, 0.0739, 0.0739, 0.0755, 0.0734, 0.0734, 0.0766, 0.0793,\n",
    "        0.0856, 0.0851, 0.0867, 0.0894, 0.0888, 0.0883, 0.0883, 0.0910, 0.0910,\n",
    "        0.0926, 0.0872, 0.0883, 0.0894, 0.0878, 0.0894, 0.0894, 0.0931, 0.0920,\n",
    "        0.0915, 0.0899, 0.0920, 0.0904, 0.0878, 0.0883, 0.0888, 0.0888, 0.0894,\n",
    "        0.0894])\n",
    "Random sortings \n",
    "tensor([0.0191, 0.0298, 0.0266, 0.0176, 0.0234, 0.0239, 0.0223, 0.0287, 0.0298,\n",
    "        0.0303, 0.0346, 0.0351, 0.0441, 0.0436, 0.0431, 0.0426, 0.0404, 0.0394,\n",
    "        0.0447, 0.0527, 0.0548, 0.0553, 0.0527, 0.0516, 0.0511, 0.0505, 0.0596,\n",
    "        0.0596, 0.0580, 0.0553, 0.0511, 0.0527, 0.0521, 0.0521, 0.0537, 0.0537,\n",
    "        0.0532, 0.0553, 0.0574, 0.0596, 0.0585, 0.0638, 0.0638, 0.0782, 0.0787,\n",
    "        0.0840, 0.0771, 0.0830, 0.0824, 0.0830, 0.0835, 0.0851, 0.0856, 0.0899,\n",
    "        0.0904, 0.0835, 0.0883, 0.0872, 0.0862, 0.0926, 0.0926, 0.0904, 0.0899,\n",
    "        0.0894])\n",
    " tensor([0.0197, 0.0303, 0.0165, 0.0330, 0.0324, 0.0340, 0.0309, 0.0309, 0.0388,\n",
    "        0.0383, 0.0356, 0.0319, 0.0367, 0.0356, 0.0404, 0.0399, 0.0351, 0.0436,\n",
    "        0.0420, 0.0457, 0.0468, 0.0580, 0.0585, 0.0590, 0.0585, 0.0606, 0.0622,\n",
    "        0.0601, 0.0681, 0.0628, 0.0660, 0.0660, 0.0654, 0.0654, 0.0654, 0.0755,\n",
    "        0.0798, 0.0824, 0.0819, 0.0798, 0.0809, 0.0862, 0.0856, 0.0851, 0.0872,\n",
    "        0.0867, 0.0872, 0.0899, 0.0931, 0.0904, 0.0878, 0.0851, 0.0888, 0.0888,\n",
    "        0.0830, 0.0846, 0.0840, 0.0840, 0.0824, 0.0819, 0.0904, 0.0926, 0.0894,\n",
    "        0.0894])\n",
    "        \n",
    " CUB\n",
    " Sorted\n",
    " tensor([0.0055, 0.0045, 0.0112, 0.0090, 0.0133, 0.0128, 0.0155, 0.0138, 0.0131,\n",
    "        0.0178, 0.0247, 0.0242, 0.0264, 0.0285, 0.0292, 0.0318, 0.0297, 0.0318,\n",
    "        0.0307, 0.0305, 0.0326, 0.0311, 0.0305, 0.0307, 0.0305, 0.0316, 0.0323,\n",
    "        0.0342, 0.0342, 0.0324, 0.0331, 0.0321, 0.0305, 0.0314, 0.0330, 0.0335,\n",
    "        0.0324, 0.0324, 0.0326, 0.0326, 0.0337, 0.0342, 0.0345, 0.0338, 0.0340,\n",
    "        0.0340, 0.0328, 0.0340, 0.0330, 0.0330, 0.0337, 0.0345, 0.0364, 0.0357,\n",
    "        0.0345, 0.0333, 0.0340, 0.0343, 0.0347, 0.0349, 0.0347, 0.0342, 0.0359,\n",
    "        0.0354])\n",
    " Random\n",
    " tensor([0.0054, 0.0091, 0.0076, 0.0093, 0.0088, 0.0091, 0.0074, 0.0066, 0.0057,\n",
    "        0.0052, 0.0069, 0.0062, 0.0040, 0.0052, 0.0052, 0.0050, 0.0050, 0.0059,\n",
    "        0.0064, 0.0069, 0.0062, 0.0060, 0.0066, 0.0085, 0.0097, 0.0102, 0.0097,\n",
    "        0.0102, 0.0104, 0.0093, 0.0240, 0.0264, 0.0283, 0.0285, 0.0281, 0.0287,\n",
    "        0.0290, 0.0290, 0.0293, 0.0309, 0.0319, 0.0300, 0.0288, 0.0307, 0.0285,\n",
    "        0.0305, 0.0328, 0.0330, 0.0326, 0.0338, 0.0314, 0.0331, 0.0316, 0.0312,\n",
    "        0.0328, 0.0328, 0.0333, 0.0338, 0.0369, 0.0385, 0.0373, 0.0364, 0.0371,\n",
    "        0.0354])\n",
    "tensor([0.0050, 0.0055, 0.0090, 0.0081, 0.0067, 0.0060, 0.0059, 0.0071, 0.0060,\n",
    "        0.0050, 0.0067, 0.0062, 0.0052, 0.0067, 0.0064, 0.0076, 0.0086, 0.0164,\n",
    "        0.0161, 0.0167, 0.0174, 0.0164, 0.0176, 0.0185, 0.0193, 0.0188, 0.0209,\n",
    "        0.0205, 0.0219, 0.0197, 0.0212, 0.0221, 0.0226, 0.0233, 0.0274, 0.0262,\n",
    "        0.0269, 0.0280, 0.0276, 0.0276, 0.0273, 0.0268, 0.0269, 0.0257, 0.0276,\n",
    "        0.0280, 0.0274, 0.0280, 0.0287, 0.0283, 0.0276, 0.0271, 0.0297, 0.0304,\n",
    "        0.0314, 0.0318, 0.0343, 0.0359, 0.0345, 0.0352, 0.0333, 0.0347, 0.0345,\n",
    "        0.0354])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49ea6786",
   "metadata": {},
   "source": [
    "0.08936170212765958 a photo of a {}\n",
    "0.096 using a photo of the texture {}\n",
    "\n",
    "MErced 17 using \"texture\" normal gets 14 amusingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = get_dataset_text_features(model, 'food101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8277ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fe115",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eecf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features.std(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31fe4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
