{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7d19f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.7/site-packages (7.7.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (7.33.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (6.13.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.8.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (5.0.9)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (62.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.17)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.17.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (2.9.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (4.5.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (4.10.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (2.15.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.11)\n",
      "Requirement already satisfied: parso>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (4.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.3)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (3.8.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.0.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.14.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Invalid requirement: 'tqdm%matplotlib'\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, CocoCaptions, ImageNet\n",
    "import slip_models\n",
    "from tokenizer import SimpleTokenizer\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "%pip install ipywidgets\n",
    "%pip install update tqdm%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7e613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Google Cloud CLI components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "Copying gs://sfr-tpu-us-east1-research/bwallace/ckpts/models/multimodel_clip/parallel_vision_standard_text/viz_parallel_text_standard_bs_1024_epoch_35.ckpt...\n",
      "==> NOTE: You are downloading one or more large file(s), which would            \n",
      "run significantly faster if you enabled sliced object downloads. This\n",
      "feature is enabled by default but requires that compiled crcmod be\n",
      "installed (see \"gsutil help crcmod\").\n",
      "\n",
      "/ [1/1 files][  1.4 GiB/  1.4 GiB] 100% Done                                    \n",
      "Operation completed over 1 objects/1.4 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!/export/home/google-cloud-sdk/bin/gsutil -m cp gs://sfr-tpu-us-east1-research/bwallace/ckpts/models/multimodel_clip/parallel_vision_standard_text/viz_parallel_text_standard_bs_1024_epoch_35.ckpt ckpts/viz_parallel_text_standard_bs_1024_epoch_35.ckpt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383b607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = T.Compose(\n",
    "                    [\n",
    "                        T.Resize(256),\n",
    "                        T.CenterCrop(224),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ]\n",
    "                        )\n",
    "\n",
    "grayscale_preprocess = T.Compose(\n",
    "                    [\n",
    "                        T.Resize(256),\n",
    "                        T.CenterCrop(224),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=[np.average([0.485, 0.456, 0.406])], std=[np.average([0.229, 0.224, 0.225])]),\n",
    "                        T.Lambda(lambda x: x.repeat(3, 1, 1) ),\n",
    "                    ]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2552cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.449], std=[0.226])\n",
       "    Lambda()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grayscale_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "578d6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = slip_models.CLIP_VITB16()\n",
    "# ckpt = torch.load('ckpts/clip_vit_epoch_35.ckpt', map_location='cpu')\n",
    "\n",
    "model = slip_models.VisionParallelTextStandard(64, 8)\n",
    "ckpt = torch.load('ckpts/viz_parallel_text_standard_bs_1024_epoch_35.ckpt', map_location='cpu')\n",
    "# ckpt = torch.load('tmpp_save/test_epoch_4.ckpt', map_location='cpu')\n",
    "\n",
    "\n",
    "# model = slip_models.CLIP_VITB16(num_prompt_tokens=64, num_text_outputs=1000)\n",
    "# ckpt = torch.load('ckpts/epoch_30_prompted_clip_may_27.ckpt', map_location='cpu')\n",
    "\n",
    "\n",
    "# model.load_state_dict({k.replace('module.',''):v for k,v in ckpt[\"model\"].items()})\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433d4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_model = slip_models.CLIP_VITB16(ssl_mlp_dim=4096, ssl_emb_dim=256)\n",
    "\n",
    "\n",
    "ckpt = torch.load('ckpts/clip_base_cc12m_35ep.pt', map_location='cpu')\n",
    "# ckpt = torch.load('ckpts/slip_base_100ep.pt', map_location='cpu')\n",
    "cc_model.load_state_dict({k.replace('module.',''):v for k,v in ckpt[\"state_dict\"].items()})\n",
    "cc_model = cc_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd0b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152dd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fff1d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# image = preprocess(Image.open(\"pics/golden-retriever.png\")).unsqueeze(0).to(device)\n",
    "# image = preprocess(Image.open(\"pics/CLIP.png\")).unsqueeze(0).to(device)\n",
    "base_text = [\"a diagram\", \"a dog\", \"a cat\"]\n",
    "model.eval()\n",
    "dog_image = preprocess(Image.open(\"pics/golden-retriever.png\"))\n",
    "diagram_image = preprocess(Image.open(\"pics/CLIP.png\").convert(\"RGB\"))\n",
    "cat_image = preprocess(Image.open(\"pics/cat.jpg\"))\n",
    "images = torch.stack([diagram_image, dog_image, cat_image]).cuda()\n",
    "\n",
    "text = tokenizer([f\"a picture of {s}\" for s in base_text]).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(images)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    # image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    # text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    \n",
    "    logits_per_image = model.logit_scale.exp() * image_features @ text_features.t()\n",
    "    # logits_per_image, logits_per_text = model(images, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe1554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# image = preprocess(Image.open(\"pics/golden-retriever.png\")).unsqueeze(0).to(device)\n",
    "# image = preprocess(Image.open(\"pics/CLIP.png\")).unsqueeze(0).to(device)\n",
    "base_text = [\"a diagram\", \"a dog\", \"a cat\"]\n",
    "model.eval()\n",
    "dog_image = preprocess(Image.open(\"pics/golden-retriever.png\"))\n",
    "diagram_image = preprocess(Image.open(\"pics/CLIP.png\").convert(\"RGB\"))\n",
    "cat_image = preprocess(Image.open(\"pics/cat.jpg\"))\n",
    "images = torch.stack([diagram_image, dog_image, cat_image]).cuda()\n",
    "\n",
    "text = tokenizer([f\"a picture of {s}\" for s in base_text]).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = cc_model.encode_image(images)\n",
    "    text_features = cc_model.encode_text(text)\n",
    "    \n",
    "    # image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    # text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    \n",
    "    logits_per_image = model.logit_scale.exp() * image_features @ text_features.t()\n",
    "    # logits_per_image, logits_per_text = model(images, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed2a0c",
   "metadata": {},
   "source": [
    "# Basic ImageNet and CIFAR checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b6734b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "32 / 10000 // Current Acc: 0.90625\n",
      "64 / 10000 // Current Acc: 0.78125\n",
      "96 / 10000 // Current Acc: 0.78125\n",
      "128 / 10000 // Current Acc: 0.7578125\n",
      "160 / 10000 // Current Acc: 0.73125\n",
      "192 / 10000 // Current Acc: 0.7083333333333334\n",
      "224 / 10000 // Current Acc: 0.6964285714285714\n",
      "256 / 10000 // Current Acc: 0.6953125\n",
      "288 / 10000 // Current Acc: 0.7013888888888888\n",
      "320 / 10000 // Current Acc: 0.690625\n",
      "352 / 10000 // Current Acc: 0.6846590909090909\n",
      "384 / 10000 // Current Acc: 0.6796875\n",
      "416 / 10000 // Current Acc: 0.6634615384615384\n",
      "448 / 10000 // Current Acc: 0.6674107142857143\n",
      "480 / 10000 // Current Acc: 0.6666666666666666\n",
      "512 / 10000 // Current Acc: 0.67578125\n",
      "544 / 10000 // Current Acc: 0.6746323529411765\n",
      "576 / 10000 // Current Acc: 0.6753472222222222\n",
      "608 / 10000 // Current Acc: 0.6825657894736842\n",
      "640 / 10000 // Current Acc: 0.6875\n",
      "672 / 10000 // Current Acc: 0.6875\n",
      "704 / 10000 // Current Acc: 0.6889204545454546\n",
      "736 / 10000 // Current Acc: 0.6902173913043478\n",
      "768 / 10000 // Current Acc: 0.6875\n",
      "800 / 10000 // Current Acc: 0.69625\n",
      "832 / 10000 // Current Acc: 0.6983173076923077\n",
      "864 / 10000 // Current Acc: 0.6944444444444444\n",
      "896 / 10000 // Current Acc: 0.6953125\n",
      "928 / 10000 // Current Acc: 0.697198275862069\n",
      "960 / 10000 // Current Acc: 0.6927083333333334\n",
      "992 / 10000 // Current Acc: 0.6965725806451613\n",
      "1024 / 10000 // Current Acc: 0.6953125\n",
      "(PARTIAL) Final Acc 0.6953125\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "cifar10 = CIFAR10(root=\"/tmp/\", transform=preprocess, download=True, train=False)\n",
    "loader = torch.utils.data.DataLoader(cifar10, batch_size=32, shuffle=True)\n",
    "# text_inputs = torch.stack([tokenizer(f\"a photo of a {c}\") for c in cifar10.classes]).cuda()\n",
    "\n",
    "\n",
    "text_inputs = torch.stack([tokenizer(f\"a photo of a {c}\") for c in cifar10.classes]).cuda()\n",
    "\n",
    "\n",
    "num_correct = 0\n",
    "num_seen = 0\n",
    "with torch.no_grad():\n",
    "    #  text_features = model.encode_text(text_inputs)\n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        # image_features = model.encode_image(imgs)\n",
    "        # results = model(imgs, text_inputs, lang_prompt_viz=True, sharded_computation=False)\n",
    "        # results = cc_model(imgs, text_inputs)\n",
    "        image_features = model.encode_image(imgs)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        \n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        # image_features = (image_features - image_features.mean(0)) / torch.sqrt(image_features.var(0))\n",
    "        # print(\"Using barlow\"); text_features = (text_features - text_features.mean(0)) / torch.sqrt(text_features.var(0)) \n",
    "        # image_features = (image_features - coco_image_mean) / torch.sqrt(coco_image_var)\n",
    "        # print(\"Using barlow\"); text_features = (text_features - coco_text_mean) / torch.sqrt(coco_text_var)  \n",
    "        \n",
    "    \n",
    "        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        max_sims = similarity.max(dim=-1)[0]\n",
    "        # print(max_sims.min(), max_sims.max(), max_sims.mean())\n",
    "        num_correct += (similarity.argmax(dim=-1)==targets).sum().item()  \n",
    "        num_seen += imgs.shape[0]\n",
    "        curr_acc = num_correct / num_seen\n",
    "        print(f\"{num_seen} / {len(cifar10)} // Current Acc: {curr_acc}\")\n",
    "        if num_seen > 1000: break\n",
    "acc = num_correct / num_seen\n",
    "print(f\"(PARTIAL) Final Acc {acc}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e35fbfd8",
   "metadata": {},
   "source": [
    "epoch 35 of my orig CLIP model (both vanilla prompting)\n",
    "CIFAR 69.2%\n",
    "ImageNEt 28.2\n",
    "\n",
    "epoch 2 of parallel vision standard text\n",
    "got 24% CIFAR  definitely nontrivial signal\n",
    "Also got 7.3% on ImageNet\n",
    "\n",
    "Epoch 15\n",
    "30 % / 3%\n",
    "\n",
    "So definitely have some overfitting (probably from removing weight decay)\n",
    "\n",
    "Epoch 21\n",
    "0.292 CIFAR\n",
    "ImageNet: only 2%, so got worse\n",
    "\n",
    "\n",
    "Potential of overfitting here? Don't have a val set, also going to load earlier ckpts\n",
    "\n",
    "\n",
    "-----\n",
    "Epoch 15 of pretrained + WD (loss ~1.14)\n",
    "10.5% ImageNet\n",
    "36.7% CIFAR10\n",
    "\n",
    "Epoch 21 (loss 0.7688705921173096) \n",
    "CIFAR10 dropped to 30.9\n",
    "ImageNet down to \n",
    "\n",
    "Epoch 35 (loss 0.14)\n",
    "\n",
    "CIFAR 32.3%\n",
    "ImageNet still 9%\n",
    "\n",
    "Epoch 35 w/ lower LR and higher WD\n",
    "~29% CIFAR\n",
    "ImageNet ~11%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fee44dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 / 50000 // Current Acc: 0.0\n",
      "64 / 50000 // Current Acc: 0.0625\n",
      "96 / 50000 // Current Acc: 0.052083333333333336\n",
      "128 / 50000 // Current Acc: 0.078125\n",
      "160 / 50000 // Current Acc: 0.09375\n",
      "192 / 50000 // Current Acc: 0.10416666666666667\n",
      "224 / 50000 // Current Acc: 0.11607142857142858\n",
      "256 / 50000 // Current Acc: 0.11328125\n",
      "288 / 50000 // Current Acc: 0.1076388888888889\n",
      "320 / 50000 // Current Acc: 0.1125\n",
      "352 / 50000 // Current Acc: 0.10795454545454546\n",
      "384 / 50000 // Current Acc: 0.10677083333333333\n",
      "416 / 50000 // Current Acc: 0.11057692307692307\n",
      "448 / 50000 // Current Acc: 0.109375\n",
      "480 / 50000 // Current Acc: 0.11458333333333333\n",
      "512 / 50000 // Current Acc: 0.111328125\n",
      "544 / 50000 // Current Acc: 0.10845588235294118\n",
      "576 / 50000 // Current Acc: 0.109375\n",
      "608 / 50000 // Current Acc: 0.1118421052631579\n",
      "640 / 50000 // Current Acc: 0.1109375\n",
      "672 / 50000 // Current Acc: 0.11160714285714286\n",
      "704 / 50000 // Current Acc: 0.11079545454545454\n",
      "736 / 50000 // Current Acc: 0.11005434782608696\n",
      "768 / 50000 // Current Acc: 0.10807291666666667\n",
      "800 / 50000 // Current Acc: 0.105\n",
      "832 / 50000 // Current Acc: 0.10697115384615384\n",
      "864 / 50000 // Current Acc: 0.1099537037037037\n",
      "896 / 50000 // Current Acc: 0.11383928571428571\n",
      "928 / 50000 // Current Acc: 0.11314655172413793\n",
      "960 / 50000 // Current Acc: 0.11458333333333333\n",
      "992 / 50000 // Current Acc: 0.11290322580645161\n",
      "1024 / 50000 // Current Acc: 0.119140625\n",
      "(PARTIAL) Final Acc 0.119140625\n"
     ]
    }
   ],
   "source": [
    "imagenet = ImageNet(root=\"/export/share/datasets/vision/imagenet\", transform=preprocess, split='val')\n",
    "loader = torch.utils.data.DataLoader(imagenet, batch_size=32, num_workers=4, shuffle=True)\n",
    "text_inputs = torch.stack([tokenizer(f\"a photo of a {c}\") for c in imagenet.classes]).cuda()\n",
    "\n",
    "num_correct = 0\n",
    "num_seen = 0\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    # print(\"Using barlow\"); text_features = (text_features - text_features.mean(0)) / torch.sqrt(text_features.var(0))\n",
    "    # print(\"Using barlow\"); text_features = (text_features - coco_text_mean) / torch.sqrt(coco_text_var)  \n",
    "    \n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        image_features = model.encode_image(imgs)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        # print(\"using barlow\"); image_features = (image_features - image_features.mean(0)) / torch.sqrt(image_features.var(0))\n",
    "        # results = model(imgs, text_inputs, lang_prompt_viz=True, sharded_computation=False)\n",
    "        # image_features = results['image_embed']\n",
    "        # text_features = results['text_embed']\n",
    "        # image_features = (image_features - coco_image_mean) / torch.sqrt(coco_image_var) \n",
    "        \n",
    "        \n",
    "        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        num_correct += (similarity.argmax(dim=-1)==targets).sum().item()  \n",
    "        max_sims = similarity.max(dim=-1)[0]\n",
    "        # print(max_sims.min(), max_sims.max(), max_sims.mean())\n",
    "        num_seen += imgs.shape[0]\n",
    "        curr_acc = num_correct / num_seen\n",
    "        print(f\"{num_seen} / {len(imagenet)} // Current Acc: {curr_acc}\")\n",
    "        if num_seen > 1000: break\n",
    "        \n",
    "print(f\"(PARTIAL) Final Acc {curr_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9564c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd182a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dataset_constructor(name, split, transform):\n",
    "    \"\"\"\n",
    "    inputs\n",
    "        name: string that we'll run if/else on\n",
    "        split: \"train\" or \"val\" for now\n",
    "    \"\"\"\n",
    "    assert split in ['train', 'val']\n",
    "\n",
    "    # Need to figure out way to override the getitem call for strong weak augmentatin\n",
    "    if name == 'cifar10':\n",
    "        # Has class names\n",
    "        return datasets.CIFAR10(root=os.path.expanduser(\"~/.cache\"),\n",
    "                                download=True,\n",
    "                                train=(split=='train'),\n",
    "                               transform=transform)\n",
    "    elif name == 'cifar100':\n",
    "        # Has class names\n",
    "        return datasets.CIFAR100(root=os.path.expanduser(\"~/.cache\"),\n",
    "                                download=True,\n",
    "                                train=(split=='train'),\n",
    "                                transform=transform)\n",
    "\n",
    "    elif name == 'svhn':\n",
    "        # Doesn't have class names\n",
    "        return datasets.SVHN(root=os.path.expanduser(\"~/.cache\"),\n",
    "                             download=True,\n",
    "                             transform=transform,\n",
    "                             split=split)\n",
    "    elif name == 'gtsrb': #####\n",
    "        assert split=='val'\n",
    "        return datasets.ImageFolder(root=\"/export/share/bwallace/datasets/gtsrb/pytorch_format\",\n",
    "                                   transform=transform)\n",
    "        return datasets.GTSRB(root=os.path.expanduser(\"~/.cache\"), split='test', download=True)\n",
    "    elif name == 'food101':\n",
    "        # name is in .classes\n",
    "        assert split=='val'\n",
    "        return datasets.ImageFolder(root=\"/export/share/bwallace/datasets/food101/images/\",\n",
    "                   transform=transform)\n",
    "    elif name == 'merced':\n",
    "        # name is in .classes\n",
    "        assert split=='val'\n",
    "        return datasets.ImageFolder(root=\"/export/share/bwallace/datasets/UCMerced_LandUse/Images/\",\n",
    "                   transform=transform)\n",
    "    elif name == 'mnist':\n",
    "        assert split=='val'\n",
    "        return datasets.MNIST(root=os.path.expanduser(\"~/.cache\"), train=False, download=True, transform=transform)\n",
    "    elif name == 'flowers': ####\n",
    "        assert split=='val'\n",
    "        print(\"NOTE PROPER METRIC IS MEAN PER CLASS\")\n",
    "        return datasets.ImageFolder(root=\"/export/share/bwallace/datasets/flowers/test/\",\n",
    "                                    transform=transform)\n",
    "    elif name == 'aircraft': ####\n",
    "        assert split=='val'\n",
    "        print(\"NOTE PROPER METRIC IS MEAN PER CLASS\")\n",
    "        return datasets.ImageFolder(root=\"/export/share/bwallace/datasets/aircraft/data/test_pytorch_format/\",\n",
    "                                   transform=transform)\n",
    "    elif name == 'cars':\n",
    "        assert split=='val'\n",
    "        return datasets.ImageFolder(root=\"/export/share/datasets/vision/stanford_cars/car_data/test/\",\n",
    "                                   transform=transform)\n",
    "    elif name == 'eurosat':\n",
    "        assert split=='val'\n",
    "        return datasets.ImageFolder(root=\"/export/share/datasets/vision/euro_sat/2750/\",\n",
    "                                   transform=transform)\n",
    "    elif name == 'dtd':\n",
    "        # name is in .classes\n",
    "        return datasets.ImageFolder(root=f\"/export/share/bwallace/datasets/dtd/{split}\",\n",
    "                   transform=transform)\n",
    "    elif name == 'cub':\n",
    "        # name is in .classes\n",
    "        return datasets.ImageFolder(root=f\"/export/share/bwallace/datasets/CUB_2011_formatted/{split}\",\n",
    "                   transform=transform)\n",
    "    elif name == 'places365':\n",
    "        # class names works in \n",
    "        return datasets.Places365(\n",
    "                    root=\"/export/share/datasets/vision/Places365/\",\n",
    "                    split='train-standard' if split=='train' else split,\n",
    "                   transform=transform)\n",
    "\n",
    "    elif name == 'imagenet':\n",
    "        # Doesn't have class names, have separate call\n",
    "        return datasets.ImageNet(\n",
    "                    root=\"/export/share/datasets/vision/imagenet/\",\n",
    "                    split=split,\n",
    "                   transform=transform)\n",
    "    elif name == 'imagenet_val':\n",
    "        assert split=='train'\n",
    "        # Doesn't have class names, only use is having Imagenet val be trainset \n",
    "        return datasets.ImageNet(\n",
    "                    root=\"/export/share/datasets/vision/imagenet/\",\n",
    "                    split='val',\n",
    "                   transform=transform)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_imagenet_class_dict():\n",
    "        idx_to_word_id_and_name_tuple = json.load(open('imagenet_class_index.json'))\n",
    "        word_id_to_name_and_idx = {v[0]:(v[1],int(k))\n",
    "                           for k,v in idx_to_word_id_and_name_tuple.items()}\n",
    "        # e.g. n0023923939 to ('unicorn', 123)\n",
    "        return word_id_to_name_and_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd4b4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_dataset(model, dataset_name, transform,\n",
    "                   prompt_template=None,\n",
    "                   test_components_mode='standard',\n",
    "                   normalization_from_all_components=False,\n",
    "                   component_grouping=1):\n",
    "    \"\"\"\n",
    "    Options for test components mode\n",
    "        standard : standard\n",
    "        test_all : scale through\n",
    "        random_all : As all but random sorting\n",
    "        \n",
    "    Normalization is whether to normalize from all components or just ones in computation\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = dataset_constructor(dataset_name, 'val', transform)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \n",
    "        if prompt_template is not None:\n",
    "            class_names = [prompt_template.format(c.replace(\"'\",\"\")) for c in dataset.classes]\n",
    "            # print(class_names)\n",
    "            text_inputs = torch.stack([tokenizer(txt) for txt in class_names]).cuda()\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "            text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        else:\n",
    "            with open('templates.json') as f:\n",
    "                all_templates = json.load(f)\n",
    "            templates = all_templates[dataset_name]\n",
    "            text_features = []\n",
    "            class_names = [c.replace(\"'\", \"\") for c in dataset.classes]\n",
    "            for class_name in class_names:\n",
    "                text_inputs = torch.stack([tokenizer(t.format(class_name)) for t in templates]).cuda()\n",
    "                class_features = model.encode_text(text_inputs)\n",
    "                class_features = class_features / class_features.norm(dim=-1, keepdim=True)\n",
    "                # print(class_features.shape)\n",
    "                class_features = class_features.mean(dim=0)\n",
    "                class_features = class_features / class_features.norm(dim=-1, keepdim=False)\n",
    "                # print(class_features.shape)\n",
    "                text_features.append(class_features)\n",
    "            text_features = torch.stack(text_features)\n",
    "            # print(text_features.shape)\n",
    "                \n",
    "        if test_components_mode in ['standard']:\n",
    "            sorted_feature_variances, sorted_variance_idx = None, None\n",
    "        elif test_components_mode in ['test_all', 'ensemble']:\n",
    "            # grouping will return indices in blocks (idx //component_grouping will be repeated numbers in blocks of component_grouping)\n",
    "            sorted_feature_variances, sorted_variance_idx = sorted_feature_component_variances(text_features,\n",
    "                                                                                              group_size=component_grouping)\n",
    "        elif test_components_mode == 'random_all':\n",
    "            # Want random to be grouped as well\n",
    "            sorted_feature_variances, sorted_variance_idx = sorted_feature_component_variances(text_features,\n",
    "                                                                                              group_size=component_grouping)\n",
    "            grouped_idx = torch.randperm(sorted_variance_idx.shape[0] // component_grouping)\n",
    "            sorted_variance_idx = torch.cat([torch.arange(component_grouping).cuda() + (component_grouping*i) for i in grouped_idx])\n",
    "            # print(sorted_variance_idx)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        num_correct = 0 if test_components_mode =='standard' else torch.zeros(text_features.shape[1])\n",
    "        num_seen = 0\n",
    "        for imgs, targets in tqdm(loader):\n",
    "            imgs = imgs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            image_features = model.encode_image(imgs)\n",
    "            image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "            # results = model(imgs, text_inputs, lang_prompt_viz=True, sharded_computation=False)\n",
    "            # image_features = results['image_embed']\n",
    "            # text_features = results['text_embed']\n",
    "\n",
    "\n",
    "            if test_components_mode==\"standard\":\n",
    "                similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "                num_correct += (similarity.argmax(dim=-1)==targets).sum().item() \n",
    "            elif test_components_mode in [\"test_all\", \"random_all\", \"ensemble\"]:\n",
    "                for num_components in range(component_grouping, text_features.shape[1]+1,\n",
    "                                           component_grouping):\n",
    "                    idx_to_use = sorted_variance_idx[:num_components]\n",
    "                    masked_text_features = text_features[:, idx_to_use]\n",
    "                    masked_image_features = image_features[:, idx_to_use]\n",
    "                    if not normalization_from_all_components:\n",
    "                        masked_text_features =  masked_text_features / masked_text_features.norm(dim=1, keepdim=True)\n",
    "                        masked_image_features =  masked_image_features / masked_image_features.norm(dim=1, keepdim=True)\n",
    "                    if test_components_mode in [\"test_all\", \"random_all\"]:\n",
    "                        masked_similarity = (masked_image_features @ masked_text_features.T).softmax(dim=-1)\n",
    "                        num_correct[num_components-1] += (masked_similarity.argmax(dim=-1)==targets).sum().item()\n",
    "                    elif test_components_mode == 'ensemble':\n",
    "                        # right now features are bs x (head_dim * num_components)\n",
    "                        # put as bs x num_components x head_dim\n",
    "                        # permute to num_components x bs x head_dim\n",
    "                        # BMM against num_components x head_dim x bs\n",
    "                        # get num_components bs x bs selection matrices\n",
    "                        # print(masked_image_features.shape)\n",
    "                        # print(masked_text_features.shape)\n",
    "                        num_texts = masked_text_features.shape[0]\n",
    "                        num_images = masked_image_features.shape[0]\n",
    "                        masked_image_features = masked_image_features.view(num_images, -1, component_grouping).permute(1, 0, 2)\n",
    "                        masked_text_features = masked_text_features.view(num_texts, -1, component_grouping).permute(1, 2, 0)\n",
    "                        per_component_sims = torch.bmm(masked_image_features, masked_text_features) # num_components x num_images x num_texts\n",
    "                        per_image_per_component_preds = per_component_sims.argmax(dim=-1) # num_components x num_images\n",
    "                        # print(per_image_per_component_preds)\n",
    "                        # print(per_image_per_component_preds.shape)\n",
    "                        ensemble_pred = per_image_per_component_preds.cpu().mode(0)[0]\n",
    "                        # print(ensemble_pred.shape, targets.shape)\n",
    "                        num_correct[num_components-1] += (ensemble_pred == targets.cpu()).sum().item()\n",
    "                        # BMM against bs x head_dim x num_components and get \n",
    "                        # bs x num_components x num_components\n",
    "            else:\n",
    "                 raise NotImplementedError\n",
    "            # max_sims = similarity.max(dim=-1)[0]\n",
    "            # print(max_sims.min(), max_sims.max(), max_sims.mean())\n",
    "            num_seen += imgs.shape[0]\n",
    "            curr_acc = num_correct / num_seen\n",
    "    return curr_acc\n",
    "\n",
    "\n",
    "def get_dataset_text_features(model, dataset_name, norm=True, print_class_names=False):\n",
    "    dataset = dataset_constructor(dataset_name, 'val', None)\n",
    "    class_names = [f\"a photo of a {c}\" for c in dataset.classes]\n",
    "    if print_class_names:\n",
    "            print(dataset.classes)\n",
    "    text_inputs = torch.stack([tokenizer(txt) for txt in class_names]).cuda()\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        if norm: text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    return text_features\n",
    "\n",
    "def sorted_feature_component_variances(tensor, group_size=1):\n",
    "    # NOTE: sorting max first so [0] is intuitively important\n",
    "    feature_variances = tensor.var(dim=0)\n",
    "    grouped_feature_variances = feature_variances.view(-1, group_size).mean(1)\n",
    "    grouped_idx = grouped_feature_variances.argsort().flip(0)\n",
    "    # will be # components / group_size\n",
    "    idx = torch.cat([torch.arange(group_size).cuda() + (group_size*i) for i in grouped_idx])\n",
    "    return feature_variances[idx], idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d77f1647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ordered_vs_random_acc_samples(ordered_accs, random_acc_sampling,\n",
    "                                       baseline_ordered_accs=None, baseline_random_acc_sampling=None,\n",
    "                                       title=None):\n",
    "    random_accs = torch.mean(torch.stack(random_acc_sampling, dim=0), dim=0)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('# components used')\n",
    "    plt.plot(ordered_accs.detach().cpu().numpy(), label=\"Sorted Variances\")\n",
    "    plt.plot(random_accs.detach().cpu().numpy(), label=f\"Random (average {len(random_acc_sampling)} runs)\")\n",
    "    if baseline_ordered_accs is not None:\n",
    "        plt.plot(baseline_ordered_accs.detach().cpu().numpy(), label=\"Baseline sorted\")\n",
    "    if baseline_random_acc_sampling is not None:\n",
    "        plt.plot(torch.stack(baseline_random_acc_sampling).mean(0).detach().cpu().numpy(), label=f\"Baseline Random (average {len(baseline_random_acc_sampling)} runs)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def generate_order_variance_plot(model, ds_name, preprocess, num_samples=2, group_size=1, global_norm=False,\n",
    "                                    baseline_model=None):\n",
    "    random_acc_sampling = [test_on_dataset(model, ds_name, preprocess,\n",
    "                                           test_components_mode='random_all',\n",
    "                                          component_grouping=group_size,\n",
    "                                          normalization_from_all_components=global_norm) for _ in range(num_samples)]\n",
    "    ordered_accs = test_on_dataset(model, ds_name, preprocess,\n",
    "                                   test_components_mode='test_all', component_grouping=group_size,\n",
    "                                          normalization_from_all_components=global_norm)\n",
    "    \n",
    "    baseline_random_acc_sampling = None\n",
    "    baseline_ordered_accs = None\n",
    "    if baseline_model is not None:\n",
    "        baseline_random_acc_sampling = [test_on_dataset(baseline_model, ds_name, preprocess,\n",
    "                                               test_components_mode='random_all',\n",
    "                                              component_grouping=group_size,\n",
    "                                              normalization_from_all_components=global_norm) for _ in range(num_samples)]\n",
    "        baseline_ordered_accs = test_on_dataset(baseline_model, ds_name, preprocess,\n",
    "                                       test_components_mode='test_all', component_grouping=group_size,\n",
    "                                              normalization_from_all_components=global_norm)\n",
    "    plot_ordered_vs_random_acc_samples(ordered_accs, random_acc_sampling, title=ds_name,\n",
    "                                      baseline_ordered_accs=baseline_ordered_accs,\n",
    "                                      baseline_random_acc_sampling=baseline_random_acc_sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2134c831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 313/313 [02:36<00:00,  1.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1084, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1084, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1297, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1495, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1159, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1351, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1321, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1467, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1601,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1683, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1564, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1711, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1829, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1856, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1855, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2042, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1792, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1730,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1819, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2014, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2068, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.2008, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1969, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2011, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2027,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1947, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1753, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1771, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1812, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1808, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1897, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1915,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1921, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1944, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1916, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.2036, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2110, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2139, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2133, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2048, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2028,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2052, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2084, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2085, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.2150, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2142, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2220, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2203, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2231, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2215,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2233, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2266, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2229, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.2190, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2213, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2212, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2259, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2254, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2223,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2217])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_on_dataset(model, 'cifar10', preprocess, test_components_mode='ensemble',\n",
    "               component_grouping=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2171de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 313/313 [01:35<00:00,  3.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2301"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_on_dataset(model, 'cifar10', preprocess, test_components_mode='standard',\n",
    "               component_grouping=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2713f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59/59 [00:12<00:00,  4.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59/59 [00:12<00:00,  4.54it/s]\n",
      " 47%|██████████████████████████████████████████████████████████████████▉                                                                          | 28/59 [00:06<00:07,  4.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_305/2407505535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_order_variance_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcc_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_305/3675420466.py\u001b[0m in \u001b[0;36mgenerate_order_variance_plot\u001b[0;34m(model, ds_name, preprocess, num_samples, group_size, global_norm, baseline_model)\u001b[0m\n\u001b[1;32m     25\u001b[0m     ordered_accs = test_on_dataset(model, ds_name, preprocess,\n\u001b[1;32m     26\u001b[0m                                    \u001b[0mtest_components_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_grouping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                           normalization_from_all_components=global_norm)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbaseline_random_acc_sampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_305/1343582370.py\u001b[0m in \u001b[0;36mtest_on_dataset\u001b[0;34m(model, dataset_name, transform, prompt_template, test_components_mode, normalization_from_all_components, component_grouping)\u001b[0m\n\u001b[1;32m     83\u001b[0m                         \u001b[0mmasked_text_features\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmasked_text_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmasked_text_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                         \u001b[0mmasked_image_features\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmasked_image_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmasked_image_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0mmasked_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmasked_image_features\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mmasked_text_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                     \u001b[0mnum_correct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_components\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmasked_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_order_variance_plot(model, 'dtd', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f56167",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'mnist', grayscale_preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6aa8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'eurosat', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c32d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'cars', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'food101', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12260507",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'cifar10', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'cifar100', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'cub', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'gtsrb', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10602c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'flowers', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_order_variance_plot(model, 'aircraft', preprocess, group_size=8, global_norm=False, baseline_model=cc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527e0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fcdf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_names = ['dtd', 'mnist', 'eurosat', 'cars', 'food101', 'cifar10',\n",
    "           'cifar100', 'cub', 'gtsrb', 'flowers', 'aircraft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16308f71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59/59 [00:05<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19414893617021275\n",
      "dtd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59/59 [00:05<00:00, 11.51it/s]\n",
      "  0%|                                                                                                                                                     | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1803191489361702\n",
      "mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:11<00:00, 27.72it/s]\n",
      "  0%|                                                                                                                                                     | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0987\n",
      "mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:11<00:00, 27.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1054\n",
      "eurosat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 844/844 [00:30<00:00, 27.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15518518518518518\n",
      "eurosat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 844/844 [00:30<00:00, 27.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22966666666666666\n",
      "cars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 252/252 [00:33<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22820544708369606\n",
      "cars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 252/252 [00:30<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23019524934709612\n",
      "food101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3157/3157 [05:06<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3615247524752475\n",
      "food101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3157/3157 [03:08<00:00, 16.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4285841584158416\n",
      "cifar10\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/.cache/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4ac66dfc49404fbfea243b66729933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting /root/.cache/cifar-10-python.tar.gz to /root/.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:11<00:00, 26.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6876\n",
      "cifar10\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:11<00:00, 27.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6987\n",
      "cifar100\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /root/.cache/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e14ba70ed548f48147055292b40830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=169001437.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting /root/.cache/cifar-100-python.tar.gz to /root/.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:11<00:00, 26.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.324\n",
      "cifar100\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:12<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3673\n",
      "cub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 182/182 [00:21<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10044874007594062\n",
      "cub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 182/182 [00:12<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12098722816706939\n",
      "gtsrb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 395/395 [00:22<00:00, 17.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09422011084718923\n",
      "gtsrb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 395/395 [00:15<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09311163895486936\n",
      "flowers\n",
      "NOTE PROPER METRIC IS MEAN PER CLASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 193/193 [00:22<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27760611481541714\n",
      "flowers\n",
      "NOTE PROPER METRIC IS MEAN PER CLASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 193/193 [00:14<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3018376971865344\n",
      "aircraft\n",
      "NOTE PROPER METRIC IS MEAN PER CLASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:29<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0195019501950195\n",
      "aircraft\n",
      "NOTE PROPER METRIC IS MEAN PER CLASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:20<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0306030603060306\n"
     ]
    }
   ],
   "source": [
    "for ds_name in ds_names:\n",
    "    for net in [model, cc_model]:\n",
    "        print(ds_name)\n",
    "        print(test_on_dataset(net, ds_name,\n",
    "                              grayscale_preprocess if ds_name=='mnist' else preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52481c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903d2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e7313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc64f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d34e4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
